# åŠ è½½åº“
from datasets import Dataset
import pandas as pd
from transformers import (AutoTokenizer,
                          AutoModelForCausalLM,
                          DataCollatorForSeq2Seq,
                          TrainingArguments,
                          Trainer,
                          GenerationConfig)
from peft import LoraConfig, TaskType, get_peft_model

DATA_PATH = 'step_data.json'
MODEL_PATH = '/home/wangjy/data/LLM/AI-ModelScope/Mistral-7B-Instruct-v0.1'

LoRA_R = 16
LoRA_ALPHA = 64
LoRA_DROPOUT = 0.05
BIAS = 'none'

# æ•°æ®é›†åŠ è½½
df = pd.read_json(DATA_PATH)
ds = Dataset.from_pandas(df)

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
# <s>[INST] ä½ å¥½å‘€ [/INST]ä½ å¥½ï¼Œä½ æœ‰ä»€ä¹ˆäº‹æƒ…è¦é—®æˆ‘å—ï¼Ÿ</s>
def process_func(example):
    MAX_LENGTH = 512
    input_ids, attention_mask, labels = [], [], []
    instruction = tokenizer(f"<s>[INST]{example['query']}[/INST]", add_special_tokens=False)
    response = tokenizer(f"{example['steps']}</s>", add_special_tokens=False)
    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [-100]
    if len(input_ids) > MAX_LENGTH:  # åšä¸€ä¸ªæˆªæ–­
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

tokenized_id = ds.map(process_func, remove_columns=ds.column_names)

model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map="cuda", torch_dtype='auto')
model.enable_input_require_grads()
print(f">>>>>>>>>> Model Dtype{model.dtype}. >>>>>>>>>>")

config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False,
    r=LoRA_R,
    lora_alpha=LoRA_ALPHA,
    lora_dropout=LoRA_DROPOUT,
    bias=BIAS
)

model = get_peft_model(model, config)

print(">>>>>>>>>> Training Parameters: ")
model.print_trainable_parameters()

training_args = TrainingArguments(
    output_dir="./Mistral-7B-Instruct-v0.1_lora_finetuned",           # æ¨¡å‹ä¿å­˜è·¯å¾„
    num_train_epochs=2,                      # â† è®ºæ–‡ï¼š2 epochs
    per_device_train_batch_size=8,           # â† å•å¡ç›´æ¥=8ï¼ˆglobal batch=8ï¼‰
    gradient_accumulation_steps=1,           # å•å¡æ˜¾å­˜å¤Ÿå°±ä¸ç´¯ç§¯
    learning_rate=1e-5,                      # â† è®ºæ–‡ï¼š1e-5
    weight_decay=1e-5,                       # â† è®ºæ–‡ï¼š1e-5
    lr_scheduler_type="cosine",              # æ¨è cosine è¡°å‡
    warmup_ratio=0.03,                       # å‰ 3% steps çƒ­èº«
    logging_steps=10,                        # æ¯10æ­¥æ‰“å°ä¸€æ¬¡loss
    save_strategy="epoch",                   # æ¯ä¸ªepochä¿å­˜ä¸€æ¬¡
    save_total_limit=2,                      # æœ€å¤šä¿ç•™2ä¸ªcheckpoint
    bf16=True,                               # è‹¥GPUæ”¯æŒï¼ˆA100/3090/4090ç­‰ï¼‰
    optim="adamw_torch",                     # æ ‡å‡†ä¼˜åŒ–å™¨
    gradient_checkpointing=True,             # âœ… èŠ‚çœæ˜¾å­˜ï¼Œå¼ºçƒˆå»ºè®®å¼€å¯
    remove_unused_columns=False,             # âœ… å¿…é¡»ï¼å¦åˆ™ labels ä¼šè¢«åˆ 
    report_to="none",                        # ä¸ç”¨ wandb/tensorboard
    seed=42,                                 # ä¿è¯å¤ç°æ€§
    dataloader_num_workers=4,                # åŠ é€Ÿæ•°æ®åŠ è½½
    dataloader_pin_memory=True,              # åŠ é€Ÿæ•°æ®ä¼ è¾“åˆ°GPU
)

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    padding=True,              # åŠ¨æ€ padding
    return_tensors="pt",       # è¿”å› PyTorch tensorï¼ˆé»˜è®¤å°±æ˜¯ï¼‰
    pad_to_multiple_of=8,      # ğŸ‘ˆ å¯¹é½åˆ° 8 çš„å€æ•°ï¼Œæå‡ bf16/amp æ•ˆç‡ï¼ˆL40S å‹å¥½ï¼ï¼‰
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_id,
    data_collator=data_collator
)

trainer.train()

final_lora_path = "./Mistral-7B-Instruct-v0.1_lora_final"
trainer.model.save_pretrained(final_lora_path)
tokenizer.save_pretrained(final_lora_path)